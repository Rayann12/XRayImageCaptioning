{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdbfe6c5-42b3-4452-a03a-d382c9521048",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = 'images'\n",
    "folder_name = ''\n",
    "tr_file_name = 'train.pkl'\n",
    "te_file_name = 'test.pkl'\n",
    "chexnet_weights = 'CheXNet_weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d102ac0-98c9-4fe0-8393-c1f95a09e6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mannu\\AppData\\Local\\Temp\\ipykernel_2052\\3412753689.py:17: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib #for saving model files as pkl files\n",
    "import os\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import imgaug.augmenters as iaa\n",
    "sns.set(palette='muted',style='white')\n",
    "from grammify import restructure\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense,GlobalAveragePooling2D, Input, Embedding, LSTM,Dot,Reshape,Concatenate,BatchNormalization, GlobalMaxPooling2D, Dropout, Add, MaxPooling2D, GRU, AveragePooling2D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "from nltk.translate.bleu_score import sentence_bleu #bleu score\n",
    "import os\n",
    "import math\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce275ca4-3787-4c43-b480-99b8b7310109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4487, 8), (563, 8))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_pickle(os.path.join(folder_name,tr_file_name))\n",
    "test = pd.read_pickle(os.path.join(folder_name,te_file_name))\n",
    "train.shape,test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccf2ee38-d426-4311-b47d-fe21667818cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The max and min value of \"caption length\" was found to be 133 and 2 respectively\n",
      "The 80 percentile value of caption_len which is 26 will be taken as the maximum padded value for each impression for faster training.\n"
     ]
    }
   ],
   "source": [
    "#tokenizer\n",
    "tokenizer = Tokenizer(filters = '',oov_token = '') #setting filters to none\n",
    "tokenizer.fit_on_texts(train.impression_final.values)\n",
    "train_captions = tokenizer.texts_to_sequences(train.impression_final) \n",
    "test_captions = tokenizer.texts_to_sequences(test.impression_final) \n",
    "vocab_size = len(tokenizer.word_index)\n",
    "caption_len = np.array([len(i) for i in train_captions])\n",
    "start_index = tokenizer.word_index[''] #tokened value of \n",
    "end_index = tokenizer.word_index[''] #tokened value of \n",
    "\n",
    "# print('\\nThe max and min value of \"caption length\" was found to be %i and %i respectively'%(max(caption_len),min(caption_len)))\n",
    "# print('The 80 percentile value of caption_len which is %i will be taken as the maximum padded value for each impression for faster training.'\n",
    "# %(np.percentile(caption_len,80)))\n",
    "max_pad = int(np.percentile(caption_len,80))\n",
    "del train_captions,test_captions #we will create tokenizing  and padding in-built in dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f99b0c15-6215-4551-9c24-f09cc48bd1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "embedding_dim = 300\n",
    "dense_dim = 512\n",
    "lstm_units = dense_dim\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ebe0533-d16d-4b33-b5e8-f8ddb0dfb056",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class Dataset():\n",
    "  #here we will get the images converted to vector form and the corresponding captions\n",
    "  def __init__(self,df,input_size,tokenizer = tokenizer, augmentation = True,max_pad = max_pad): \n",
    "    \"\"\"\n",
    "    df  = dataframe containing image_1,image_2 and impression\n",
    "    \"\"\"\n",
    "    self.image1 = df.image_1\n",
    "    self.image2 = df.image_2\n",
    "    self.caption = df.impression_ip #inp\n",
    "    self.caption1 = df.impression_op  #output\n",
    "    self.input_size = input_size #tuple ex: (512,512)\n",
    "    self.tokenizer = tokenizer\n",
    "    self.augmentation = augmentation\n",
    "    self.max_pad = max_pad\n",
    "    self.df = df\n",
    "\n",
    "    #image augmentation\n",
    "    #https://imgaug.readthedocs.io/en/latest/source/overview/flip.html?highlight=Fliplr\n",
    "    self.aug1 = iaa.Fliplr(1) #flip images horizaontally\n",
    "    self.aug2 = iaa.Flipud(1) #flip images vertically\n",
    "\n",
    "    # https://imgaug.readthedocs.io/en/latest/source/overview/convolutional.html?highlight=emboss#emboss\n",
    "    # self.aug3 = iaa.Emboss(alpha=(1), strength=1) #embosses image\n",
    "\n",
    "    # #https://imgaug.readthedocs.io/en/latest/source/api_augmenters_convolutional.html?highlight=sharpen#imgaug.augmenters.convolutional.Sharpen\n",
    "    # self.aug4 = iaa.Sharpen(alpha=(1.0), lightness=(1.5)) #sharpens the image and apply some lightness/brighteness 1 means fully sharpened etc\n",
    "  def get_config(self):\n",
    "    config = super().get_config()\n",
    "    config.update({\"df\": self.df, \"input_size\": self.input_size, \"tokenizer\": self.tokenizer, \"augmentation\": self.augmentation, \"max_pad\": self.max_pad})\n",
    "    return config\n",
    "  def __getitem__(self,i):\n",
    "    #gets the datapoint at i th index, we will extract the feature vectors of images after resizing the image  and apply augmentation\n",
    "    image1 = cv2.imread(self.image1[i], cv2.IMREAD_UNCHANGED)/255\n",
    "    image2 = cv2.imread(self.image2[i], cv2.IMREAD_UNCHANGED)/255\n",
    "    image1 = cv2.resize(image1,self.input_size,interpolation = cv2.INTER_NEAREST)\n",
    "    image2 = cv2.resize(image2,self.input_size,interpolation = cv2.INTER_NEAREST)\n",
    "    if image1.any()==None:\n",
    "      print(\"%i , %s image sent null value\"%(i,self.image1[i]))\n",
    "    if image2.any()==None:\n",
    "      print(\"%i , %s image sent null value\"%(i,self.image2[i]))\n",
    "\n",
    "\n",
    "    #tokenizing and padding\n",
    "    caption = self.tokenizer.texts_to_sequences(self.caption[i:i+1]) #the input should be an array for tokenizer ie [self.caption[i]] \n",
    "\n",
    "    caption = pad_sequences(caption,maxlen = self.max_pad,padding = 'post') #opshape:(input_length,)\n",
    "    caption = tf.squeeze(caption,axis=0) #opshape = (input_length,) removing unwanted axis if present\n",
    "\n",
    "    caption1 = self.tokenizer.texts_to_sequences(self.caption1[i:i+1]) #the input should be an array for tokenizer ie [self.caption[i]] \n",
    "\n",
    "    caption1 = pad_sequences(caption1,maxlen = self.max_pad,padding = 'post') #opshape: (input_length,)\n",
    "    caption1 = tf.squeeze(caption1,axis=0) #opshape = (input_length,) removing unwanted axis if present\n",
    "\n",
    "    if self.augmentation: #we will not apply augmentation that crops the image \n",
    "          a = np.random.uniform()\n",
    "          if a<0.333:\n",
    "              image1 = self.aug1.augment_image(image1)\n",
    "              image2 = self.aug1.augment_image(image2)\n",
    "          elif a<0.667:\n",
    "              image1 = self.aug2.augment_image(image1)\n",
    "              image2 = self.aug2.augment_image(image2)\n",
    "          else: #applying no augmentation\n",
    "            pass;\n",
    "\n",
    "    return tuple([image1,image2,caption,caption1])\n",
    "\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.image1)\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "class Dataloader(tf.keras.utils.Sequence):     #for batching\n",
    "    def __init__(self, dataset, batch_size=1, shuffle=True):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(self.dataset))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # collect batch data\n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "        indexes = [self.indexes[j] for j in range(start,stop)] #getting the shuffled index values\n",
    "        data = [self.dataset[j] for j in indexes] #taken from Data class (calls __getitem__ of Data) here the shape is batch_size*3, (image_1,image_2,caption)\n",
    "        batch = [np.stack(samples, axis=0) for samples in zip(*data)] #here the shape will become batch_size*input_size(of image)*3,batch_size*input_size(of image)*3\n",
    "                                                                      #,batch_size*1*max_pad\n",
    "        return tuple([tuple([batch[0],batch[1],batch[2]]),batch[3]]) #here [image1,image2, caption(without )],caption(without ) (op)\n",
    "    \n",
    "    def __len__(self): #returns total number of batches in an epoch\n",
    "        return len(self.indexes) // self.batch_size\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"dataset\": self.dataset, \"batch_size\": self.batch_size, \"shuffle\": self.shuffle})\n",
    "        return config\n",
    "    def on_batch_end(self): #it runs at the end of epoch\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes) #in-place shuffling takes place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ba5ada1-3f1c-4a9f-a6bf-7b512130c2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (224,224)\n",
    "train_dataloader = Dataset(train,input_size)\n",
    "train_dataloader = Dataloader(train_dataloader,batch_size = batch_size)\n",
    "\n",
    "test_dataloader = Dataset(test,input_size)\n",
    "test_dataloader = Dataloader(test_dataloader,batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef1dd11-2e4b-44b7-8a27-289bc080b21b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "501e01d2-bd3f-4b6e-ad87-557ad78f8bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chexnet weights ; https://drive.google.com/file/d/19BllaOvs2x5PLV_vlWMy4i8LapLb2j6b/view\n",
    "def create_chexnet(chexnet_weights = chexnet_weights,input_size = input_size):\n",
    "  \"\"\"\n",
    "  chexnet_weights: weights value in .h5 format of chexnet\n",
    "  creates a chexnet model with preloaded weights present in chexnet_weights file\n",
    "  \"\"\"\n",
    "  model = tf.keras.applications.DenseNet121(include_top=False,input_shape = input_size+(3,)) #importing densenet the last layer will be a relu activation layer\n",
    "\n",
    "  #we need to load the weights so setting the architecture of the model as same as the one of the chexnet\n",
    "  x = model.output #output from chexnet\n",
    "  x = GlobalAveragePooling2D()(x)\n",
    "  x = Dense(14, activation=\"sigmoid\", name=\"chexnet_output\")(x) #here activation is sigmoid as seen in research paper\n",
    "\n",
    "  chexnet = tf.keras.Model(inputs = model.input,outputs = x)\n",
    "  chexnet.load_weights(chexnet_weights)\n",
    "  chexnet = tf.keras.Model(inputs = model.input,outputs = chexnet.layers[-3].output)  #we will be taking the 3rd last layer (here it is layer before global avgpooling)\n",
    "  #since we are using attention here\n",
    "  return chexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d9fc0cd-d53f-460b-b38f-7f16aaaad4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "dense_dim = 512\n",
    "lstm_units = dense_dim\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db8cbaf0-740a-473f-9c40-0ef4dcc59665",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class image_encoder(tf.keras.layers.Layer):\n",
    "  \"\"\"\n",
    "  This layer will output image backbone features after passing it through chexnet\n",
    "  here chexnet will be not be trainable\n",
    "  \"\"\"\n",
    "  def __init__(self,\n",
    "               name = \"image_encoder\"\n",
    "               ):\n",
    "    super().__init__()\n",
    "    self.chexnet = create_chexnet()\n",
    "    self.chexnet.trainable = False\n",
    "    self.avgpool = AveragePooling2D(pool_size = (2,2))\n",
    "    self.name = name\n",
    "    # for i in range(10): #the last 10 layers of chexnet will be trained\n",
    "    #   self.chexnet.layers[-i].trainable = True\n",
    "  def get_config(self):\n",
    "      config = super().get_config()\n",
    "      config.update({\"name\": self.name})\n",
    "      return config\n",
    "  def call(self,data):\n",
    "    op = self.chexnet(data) #op shape: (None,7,7,1024)\n",
    "    op = self.avgpool(op) #op shape (None,3,3,1024)\n",
    "    op = tf.reshape(op,shape = (-1,op.shape[1]*op.shape[2],op.shape[3])) #op shape: (None,9,1024)\n",
    "    return op "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1377c46-8fc9-42e2-bdf0-3ccd94056119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(image1,image2,dense_dim = dense_dim,dropout_rate = dropout_rate):\n",
    "  \"\"\"\n",
    "  Takes image1,image2\n",
    "  gets the final encoded vector of these\n",
    "  \"\"\"\n",
    "  #image1\n",
    "  im_encoder = image_encoder()\n",
    "  bkfeat1 = im_encoder(image1) #shape: (None,9,1024)\n",
    "  bk_dense = Dense(dense_dim,name = 'bkdense',activation = 'relu') #shape: (None,9,512)\n",
    "  bkfeat1 = bk_dense(bkfeat1)\n",
    "\n",
    "  #image2\n",
    "  bkfeat2 = im_encoder(image2) #shape: (None,9,1024)\n",
    "  bkfeat2 = bk_dense(bkfeat2) #shape: (None,9,512)\n",
    "\n",
    "\n",
    "  #combining image1 and image2\n",
    "  concat = Concatenate(axis=1)([bkfeat1,bkfeat2]) #concatenating through the second axis shape: (None,18,1024)\n",
    "  bn = BatchNormalization(name = \"encoder_batch_norm\")(concat) \n",
    "  dropout = Dropout(dropout_rate,name = \"encoder_dropout\")(bn)\n",
    "  return dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62c81a6c-5ece-4b71-87f3-b38a0580b068",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class global_attention(tf.keras.layers.Layer):\n",
    "  \"\"\"\n",
    "  calculate global attention\n",
    "  \"\"\"\n",
    "  def __init__(self,dense_dim = dense_dim):\n",
    "    super().__init__()\n",
    "    self.dense_dim = dense_dim\n",
    "    # Intialize variables needed for Concat score function here\n",
    "    self.W1 = Dense(units = dense_dim) #weight matrix of shape enc_units*dense_dim\n",
    "    self.W2 = Dense(units = dense_dim) #weight matrix of shape dec_units*dense_dim\n",
    "    self.V = Dense(units = 1) #weight matrix of shape dense_dim*1 \n",
    "      #op (None,98,1)\n",
    "\n",
    "  def get_config(self):\n",
    "      config = super().get_config()\n",
    "      config.update({\"dense_dim\": self.dense_dim})\n",
    "      return config\n",
    "  def call(self,encoder_output,decoder_h): #here the encoded output will be the concatted image bk features shape: (None,98,dense_dim)\n",
    "    decoder_h = tf.expand_dims(decoder_h,axis=1) #shape: (None,1,dense_dim)\n",
    "    tanh_input = self.W1(encoder_output) + self.W2(decoder_h) #ouput_shape: batch_size*98*dense_dim\n",
    "    tanh_output =  tf.nn.tanh(tanh_input)\n",
    "    attention_weights = tf.nn.softmax(self.V(tanh_output),axis=1) #shape= batch_size*98*1 getting attention alphas\n",
    "    op = attention_weights*encoder_output#op_shape: batch_size*98*dense_dim  multiply all aplhas with corresponding context vector\n",
    "    context_vector = tf.reduce_sum(op,axis=1) #summing all context vector over the time period ie input length, output_shape: batch_size*dense_dim\n",
    "\n",
    "\n",
    "    return context_vector,attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55ed7635-c817-4c83-9413-845ffb15cec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = {}\n",
    "with open('glove.6B.300d.txt',encoding='utf-8') as f: #taking 300 dimesions\n",
    "  for line in f:\n",
    "    word = line.split() #it is stored as string like this \"'the': '.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.4\"\n",
    "    glove[word[0]] = np.asarray(word[1:], dtype='float32')\n",
    "\n",
    "\n",
    "embedding_dim = 300\n",
    "# create a weight matrix for words in training docs for embedding purpose\n",
    "embedding_matrix = np.zeros((1386+1, embedding_dim)) #https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "  embedding_vector = glove.get(word)\n",
    "  if embedding_vector is not None: #if the word is found in glove vectors\n",
    "      embedding_matrix[i] = embedding_vector[:embedding_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bee6eb56-abfa-4cf1-b464-bf5ae9334033",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class One_Step_Decoder(tf.keras.layers.Layer):\n",
    "  \"\"\"\n",
    "  decodes a single token\n",
    "  \"\"\"\n",
    "  def __init__(self,vocab_size = 1386, embedding_dim = embedding_dim, max_pad = max_pad, dense_dim = dense_dim ,name = \"onestepdecoder\"):\n",
    "    # Initialize decoder embedding layer, LSTM and any other objects needed\n",
    "    super().__init__()\n",
    "    self.dense_dim = dense_dim\n",
    "    self.embedding = Embedding(input_dim = 1386+1,\n",
    "                                output_dim = embedding_dim,\n",
    "                                embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "                                mask_zero=True, \n",
    "                                name = 'onestepdecoder_embedding'\n",
    "                              )\n",
    "    self.LSTM = GRU(units=self.dense_dim,\n",
    "                    # return_sequences=True,\n",
    "                    return_state=True,\n",
    "                    name = 'onestepdecoder_LSTM'\n",
    "                    )\n",
    "    self.attention = global_attention(dense_dim = dense_dim)\n",
    "    self.concat = Concatenate(axis=-1)\n",
    "    self.dense = Dense(dense_dim,name = 'onestepdecoder_embedding_dense',activation = 'relu')\n",
    "    self.final = Dense(1386+1,activation='softmax')\n",
    "    self.concat = Concatenate(axis=-1)\n",
    "    self.add =Add()\n",
    "    self.vocab_size = vocab_size\n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.max_pad = max_pad\n",
    "    self.dense_dim = dense_dim\n",
    "    self.name = name\n",
    "  def get_config(self):\n",
    "      config = super().get_config()\n",
    "      config.update({\"vocab_size\": self.vocab_size, \"embedding_dim\": self.embedding_dim, \"max_pad\": self.max_pad, \"dense_dim\": self.dense_dim, \"name\": self.name})\n",
    "      return config\n",
    "  def call(self,input_to_decoder, encoder_output, decoder_h):#,decoder_c):\n",
    "    '''\n",
    "        One step decoder mechanisim step by step:\n",
    "      A. Pass the input_to_decoder to the embedding layer and then get the output(batch_size,1,embedding_dim)\n",
    "      B. Using the encoder_output and decoder hidden state, compute the context vector.\n",
    "      C. Concat the context vector with the step A output\n",
    "      D. Pass the Step-C output to LSTM/GRU and get the decoder output and states(hidden and cell state)\n",
    "      E. Pass the decoder output to dense layer(vocab size) and store the result into output.\n",
    "      F. Return the states from step D, output from Step E, attention weights from Step -B\n",
    "\n",
    "      here state_h,state_c are decoder states\n",
    "    '''\n",
    "    embedding_op = self.embedding(input_to_decoder) #output shape = batch_size*1*embedding_shape (only 1 token)\n",
    "\n",
    "    context_vector,attention_weights = self.attention(encoder_output,decoder_h) #passing hidden state h of decoder and encoder output\n",
    "    #context_vector shape: batch_size*dense_dim we need to add time dimension\n",
    "    context_vector_time_axis = tf.expand_dims(context_vector,axis=1)\n",
    "    #now we will combine attention output context vector with next word input to the lstm here we will be teacher forcing\n",
    "    concat_input = self.concat([context_vector_time_axis,embedding_op])#output dimension = batch_size*input_length(here it is 1)*(dense_dim+embedding_dim)\n",
    "    \n",
    "    output,decoder_h = self.LSTM(concat_input,initial_state = decoder_h)\n",
    "    #output shape = batch*1*dense_dim and decoder_h,decoder_c has shape = batch*dense_dim\n",
    "    #we need to remove the time axis from this decoder_output\n",
    "    \n",
    "\n",
    "    output = self.final(output)#shape = batch_size*decoder vocab size\n",
    "    return output,decoder_h,attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5386d31d-937b-459c-b3b8-27e8968a68f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class decoder(tf.keras.Model):\n",
    "  \"\"\"\n",
    "  Decodes the encoder output and caption\n",
    "  \"\"\"\n",
    "  def __init__(self,max_pad = max_pad, embedding_dim = embedding_dim,dense_dim = dense_dim,score_fun='general',batch_size = batch_size,vocab_size = 1386):\n",
    "    super().__init__()\n",
    "    self.onestepdecoder = One_Step_Decoder(vocab_size = 1386, embedding_dim = embedding_dim, max_pad = max_pad, dense_dim = dense_dim)\n",
    "    self.output_array = tf.TensorArray(tf.float32,size=max_pad)\n",
    "    self.max_pad = max_pad\n",
    "    self.batch_size = batch_size\n",
    "    self.dense_dim =dense_dim\n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.score_fun  = score_fun\n",
    "    self.vocab_size = vocab_size\n",
    "\n",
    "  def get_config(self):\n",
    "      config = super().get_config()\n",
    "      config.update({\"embedding_dim\": self.embedding_dim, \"max_pad\": self.max_pad, \"dense_dim\": self.dense_dim, \"score_fun\": self.score_fun, \"batch_size\": self.batch_size, \"vocab_size\": self.vocab_size})\n",
    "      return config\n",
    "  def call(self,encoder_output,caption):#,decoder_h,decoder_c): #caption : (None,max_pad), encoder_output: (None,dense_dim)\n",
    "    decoder_h, decoder_c = tf.zeros_like(encoder_output[:,0]), tf.zeros_like(encoder_output[:,0]) #decoder_h, decoder_c\n",
    "    output_array = tf.TensorArray(tf.float32,size=max_pad)\n",
    "    for timestep in range(self.max_pad): #iterating through all timesteps ie through max_pad\n",
    "      output,decoder_h,attention_weights = self.onestepdecoder(caption[:,timestep:timestep+1], encoder_output, decoder_h)\n",
    "      output_array = output_array.write(timestep,output) #timestep*batch_size*vocab_size\n",
    "\n",
    "    self.output_array = tf.transpose(output_array.stack(),[1,0,2]) #.stack :Return the values in the TensorArray as a stacked Tensor.)\n",
    "        #shape output_array: (batch_size,max_pad,vocab_size)\n",
    "    return self.output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "840f4cbf-ac8e-45b3-98aa-c5b07c57ae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model creation\n",
    "tf.keras.backend.clear_session()\n",
    "image1 = Input(shape = (input_size + (3,))) #shape = 224,224,3\n",
    "image2 = Input(shape = (input_size + (3,))) #https://www.w3resource.com/python-exercises/tuple/python-tuple-exercise-5.php\n",
    "caption = Input(shape = (max_pad,))\n",
    "\n",
    "encoder_output = encoder(image1,image2) #shape: (None,28,512)\n",
    "\n",
    "output = decoder()(encoder_output,caption)\n",
    "\n",
    "model = tf.keras.Model(inputs = [image1,image2,caption], outputs = output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fa9c1bc-c5ae-489e-afc8-b5efa4cff047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)       │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)       │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ image_encoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">image_encoder</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">7,037,504</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],         │\n",
       "│                               │                           │                 │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ bkdense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │ image_encoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
       "│                               │                           │                 │ image_encoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bkdense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],             │\n",
       "│                               │                           │                 │ bkdense[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ encoder_batch_norm            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ encoder_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_batch_norm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ decoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">decoder</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1387</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">3,690,192</span> │ encoder_dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│                               │                           │                 │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)       │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)       │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ image_encoder (\u001b[38;5;33mimage_encoder\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │       \u001b[38;5;34m7,037,504\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],         │\n",
       "│                               │                           │                 │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ bkdense (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m524,800\u001b[0m │ image_encoder[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
       "│                               │                           │                 │ image_encoder[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │ bkdense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],             │\n",
       "│                               │                           │                 │ bkdense[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ encoder_batch_norm            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │           \u001b[38;5;34m2,048\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)          │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ encoder_dropout (\u001b[38;5;33mDropout\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │ encoder_batch_norm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ decoder (\u001b[38;5;33mdecoder\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m1387\u001b[0m)          │       \u001b[38;5;34m3,690,192\u001b[0m │ encoder_dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│                               │                           │                 │ input_layer_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,254,544</span> (42.93 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,254,544\u001b[0m (42.93 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,216,016</span> (16.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,216,016\u001b[0m (16.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,038,528</span> (26.85 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m7,038,528\u001b[0m (26.85 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13cfd4f1-f532-4c4c-987d-58b8c66f1f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = tf.keras.losses.SparseCategoricalCrossentropy() \n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    #getting mask value to not consider those words which are not present in the true caption\n",
    "    mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
    "\n",
    "    #y_pred = y_pred+10**-7 #to prevent loss becoming null\n",
    "\n",
    "    #calculating the loss\n",
    "    loss_ = loss_func(y_true, y_pred)\n",
    "    \n",
    "    #converting mask dtype to loss_ dtype\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    \n",
    "    #applying the mask to loss\n",
    "    loss_ = loss_*mask\n",
    "    \n",
    "    #returning mean over all the values\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "lr = 10**-2\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = lr)   \n",
    "model.compile(optimizer=optimizer,loss=custom_loss,metrics= ['accuracy'])\n",
    "# model.compile(optimizer=optimizer,loss=tf.keras.losses.SparseCategoricalCrossentropy() ,metrics= ['accuracy'])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c742a0c-14ec-43dd-a583-8c8815e2567a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tb_filename = ''\n",
    "tb_file = os.path.join('',tb_filename)\n",
    "model_filename = 'Encoder_Decoder_Weights.weights.h5'\n",
    "model_save = os.path.join('',model_filename)\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience = 5,\n",
    "                                     verbose = 2\n",
    "                                     ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=model_save,\n",
    "                                       save_best_only = True,\n",
    "                                      save_weights_only = True,\n",
    "                                       verbose = 2\n",
    "                                       ),\n",
    "    tf.keras.callbacks.TensorBoard(histogram_freq=1,\n",
    "                                   log_dir=tb_file),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=2, min_lr=10**-7, verbose = 2)\n",
    "                                   \n",
    "] #from keras documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "394b92b5-3010-4b20-b2b8-c7adc3413957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - accuracy: 0.3677 - loss: 2.4855 \n",
      "Epoch 1: val_loss improved from inf to 0.75897, saving model to Encoder_Decoder_global_attention.weights.h5\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m741s\u001b[0m 14s/step - accuracy: 0.3710 - loss: 2.4717 - val_accuracy: 0.8083 - val_loss: 0.7590 - learning_rate: 0.0100\n",
      "Epoch 2/5\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55s/step - accuracy: 0.8842 - loss: 0.4495 \n",
      "Epoch 2: val_loss improved from 0.75897 to 0.17520, saving model to Encoder_Decoder_global_attention.weights.h5\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2433s\u001b[0m 56s/step - accuracy: 0.8850 - loss: 0.4465 - val_accuracy: 0.9661 - val_loss: 0.1752 - learning_rate: 0.0100\n",
      "Epoch 3/5\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - accuracy: 0.9482 - loss: 0.2183 \n",
      "Epoch 3: val_loss improved from 0.17520 to 0.13978, saving model to Encoder_Decoder_global_attention.weights.h5\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m669s\u001b[0m 15s/step - accuracy: 0.9484 - loss: 0.2177 - val_accuracy: 0.9768 - val_loss: 0.1398 - learning_rate: 0.0100\n",
      "Epoch 4/5\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - accuracy: 0.9838 - loss: 0.0852 \n",
      "Epoch 4: val_loss improved from 0.13978 to 0.11504, saving model to Encoder_Decoder_global_attention.weights.h5\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m604s\u001b[0m 13s/step - accuracy: 0.9838 - loss: 0.0847 - val_accuracy: 0.9828 - val_loss: 0.1150 - learning_rate: 0.0100\n",
      "Epoch 5/5\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - accuracy: 0.9934 - loss: 0.0388 \n",
      "Epoch 5: val_loss improved from 0.11504 to 0.08058, saving model to Encoder_Decoder_global_attention.weights.h5\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m587s\u001b[0m 13s/step - accuracy: 0.9935 - loss: 0.0386 - val_accuracy: 0.9877 - val_loss: 0.0806 - learning_rate: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x17bfd63fbd0>"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(train_dataloader,\n",
    "#           validation_data = test_dataloader,\n",
    "#           epochs = 5,\n",
    "#           callbacks = my_callbacks\n",
    "#           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b8d4b20-364d-439a-ab59-ce88229f3425",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'FinalWeights.weights.h5'\n",
    "model_save = os.path.join('',model_filename)\n",
    "model.save_weights(model_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "165a491c-de63-47a7-a294-cdad7dfd86c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bleu(reference,prediction):\n",
    "  \"\"\"\n",
    "  Given a reference and prediction string, outputs the 1-gram,2-gram,3-gram and 4-gram bleu scores\n",
    "  \"\"\"\n",
    "  reference = [reference.split()] #should be in an array (cos of multiple references can be there here only 1)\n",
    "  prediction = prediction.split()\n",
    "  bleu1 = sentence_bleu(reference,prediction,weights = (1,0,0,0))\n",
    "  bleu2 = sentence_bleu(reference,prediction,weights = (0.5,0.5,0,0))\n",
    "  bleu3 = sentence_bleu(reference,prediction,weights = (0.33,0.33,0.33,0))\n",
    "  bleu4 = sentence_bleu(reference,prediction,weights = (0.25,0.25,0.25,0.25))\n",
    "\n",
    "  return bleu1,bleu2,bleu3,bleu4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1bc5c05e-c92c-4758-a2e1-b17e0e8745bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate bleu scores for every datapoint\n",
    "def mean_bleu(test,predict,model=model,**kwargs):\n",
    "  \"\"\"\n",
    "  given a df and predict fucntion which predicts the impression of the caption\n",
    "  outpus the mean bleu1,bleu2,bleu3, bleu4 for entire datapoints in df\n",
    "  \"\"\"\n",
    "  if kwargs!=None:\n",
    "    top_k = kwargs.get('top_k')\n",
    "  else:\n",
    "    top_k = None\n",
    "  bleu1,bleu2,bleu3,bleu4 = [],[],[],[]\n",
    "  for index, data in test.iterrows():\n",
    "    if top_k==None:\n",
    "      predict_val = predict(data['image_1'],data['image_2'],model = model) #predicted sentence\n",
    "    else:\n",
    "      predict_val = predict(data['image_1'],data['image_2'],model = model,top_k = top_k)\n",
    "    true = data.impression\n",
    "    _ = get_bleu(true,predict_val)\n",
    "    bleu1.append(_[0])\n",
    "    bleu2.append(_[1])\n",
    "    bleu3.append(_[2])\n",
    "    bleu4.append(_[3])\n",
    "  return np.array(bleu1).mean(),np.array(bleu2).mean(),np.array(bleu3).mean(),np.array(bleu4).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0dd4903e-25e3-440b-8b5b-10cadf1ab46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_search_predict(image1, image2, model=model, weights_file='Encoder_Decoder_Weights.h5'):\n",
    "    \n",
    "    # Rest of the function remains the same\n",
    "    image1 = cv2.imread(image1, cv2.IMREAD_UNCHANGED) / 255\n",
    "    image2 = cv2.imread(image2, cv2.IMREAD_UNCHANGED) / 255\n",
    "    image1 = tf.expand_dims(cv2.resize(image1, input_size, interpolation=cv2.INTER_NEAREST), axis=0)\n",
    "    image2 = tf.expand_dims(cv2.resize(image2, input_size, interpolation=cv2.INTER_NEAREST), axis=0)\n",
    "    model.load_weights('Encoder_Decoder_Weights.h5')\n",
    "    image1 = model.get_layer('image_encoder')(image1)\n",
    "    image2 = model.get_layer('image_encoder')(image2)\n",
    "    image1 = model.get_layer('bkdense')(image1)\n",
    "    image2 = model.get_layer('bkdense')(image2)\n",
    "\n",
    "    concat = model.get_layer('concatenate')([image1, image2])\n",
    "    enc_op = model.get_layer('encoder_batch_norm')(concat)\n",
    "    enc_op = model.get_layer('encoder_dropout')(enc_op)\n",
    "\n",
    "    decoder_h, decoder_c = tf.zeros_like(enc_op[:, 0]), tf.zeros_like(enc_op[:, 0])\n",
    "    a = []\n",
    "    pred = []\n",
    "    for i in range(max_pad):\n",
    "        if i == 0:\n",
    "            caption = np.array(tokenizer.texts_to_sequences(['0']))\n",
    "        output, decoder_h, attention_weights = model.get_layer('decoder').onestepdecoder(caption, enc_op, decoder_h)\n",
    "\n",
    "        max_prob = tf.argmax(output, axis=-1)\n",
    "        caption = np.array([max_prob])\n",
    "        if max_prob == np.squeeze(tokenizer.texts_to_sequences(['0'])):\n",
    "            break\n",
    "        else:\n",
    "            a.append(tf.squeeze(max_prob).numpy())\n",
    "    return restructure(tokenizer.sequences_to_texts([a])[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d99eeceb-bf0a-403c-908e-252c9dd76217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "images\\CXR1724_IM-0478-1001.png images\\CXR1724_IM-0478-1001.png\n",
      "With demonstrated midlung rib kyphosis, appropriate of and/or cardiomegaly. The heart silhouette sparing acuity.\n",
      "CPU times: total: 375 ms\n",
      "Wall time: 4.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "k = -1\n",
    "image1,image2 = test.image_1.iloc[k],test.image_2.iloc[k]\n",
    "print(greedy_search_predict(image1,image2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "96fe157e-fd53-4ee1-873d-60f2b0409692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = mean_bleu(test,greedy_search_predict)\n",
    "# k = list(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26a646b4-dbb9-4253-bad1-686c35383f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bleu1</th>\n",
       "      <th>bleu2</th>\n",
       "      <th>bleu3</th>\n",
       "      <th>bleu4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>greedy search</th>\n",
       "      <td>0.327824</td>\n",
       "      <td>0.342517</td>\n",
       "      <td>0.312943</td>\n",
       "      <td>0.356831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  bleu1     bleu2     bleu3     bleu4\n",
       "greedy search  0.327824  0.342517  0.312943  0.356831"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index = 'greedy search'\n",
    "# result = pd.DataFrame([k],columns = [\"bleu1\",\"bleu2\",\"bleu3\",\"bleu4\"],index = [index])\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "07918e6d-d8e8-4ad0-be52-ec7791d9b989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encoder_op(image1,image2,model = model1):\n",
    "#   \"\"\"\n",
    "#   Given image1 and image2 filepath, outputs\n",
    "#   their backbone features which will be input\n",
    "#   to the decoder\n",
    "#   \"\"\"\n",
    "#   image1 = cv2.imread(image1,cv2.IMREAD_UNCHANGED)/255 \n",
    "#   image2 = cv2.imread(image2,cv2.IMREAD_UNCHANGED)/255 \n",
    "#   image1 = tf.expand_dims(cv2.resize(image1,input_size,interpolation = cv2.INTER_NEAREST),axis=0) #introduce batch and resize\n",
    "#   image2 = tf.expand_dims(cv2.resize(image2,input_size,interpolation = cv2.INTER_NEAREST),axis=0)\n",
    "  \n",
    "#   image1 = model.get_layer('image_encoder')(image1) #output from chexnet\n",
    "#   image1 = model.get_layer('bkdense')(image1)\n",
    "#   image2 = model.get_layer('image_encoder')(image2)\n",
    "#   image2 = model.get_layer('bkdense')(image2)\n",
    "\n",
    "#   concat = model.get_layer('concatenate')([image1,image2])\n",
    "#   concat = model.get_layer('encoder_batch_norm')(concat)\n",
    "#   concat = model.get_layer('encoder_dropout')(concat)\n",
    "#   return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "b9eeb7ef-9f17-4737-af96-d113a2dbeafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 22s\n",
      "Wall time: 18min 57s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# test['bleu_1_gs'] = np.zeros(test.shape[0]) #greedy search\n",
    "# test['prediction_gs'] = np.zeros(test.shape[0]) #greedy search\n",
    "\n",
    "# for index,rows in test.iterrows():\n",
    "#   #greedy search\n",
    "#   predicted_text = greedy_search_predict(rows.image_1,rows.image_2,model1)\n",
    "#   test.loc[index,'prediction_gs'] = predicted_text\n",
    "#   reference = [rows['impression'].split()]\n",
    "#   test.loc[index,'bleu_1_gs'] = sentence_bleu(reference,predicted_text.split(),weights = (1,0,0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "2d68ef23-3cb8-45f5-87a6-663c18721483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prediction_gs\n",
       "disease identified evidence cardiopulmonary . left followup cm middle midlung low contours . scapula post abnormal marked indicate . chronic heart of changes pulmonary changes .                  6.039076\n",
       ". chronic heart of mediastinal well vasculature opacities . of changes . frontal entirely area with upper development grade visualized without overt perihilar degenerative cardiopulmonary the    4.973357\n",
       ". of mediastinal opacities chest effusion pulmonary changes . the left upper lung correlation lower cardiac degenerative on . the left upper lung correlation lower cardiac                        3.374778\n",
       ". of mediastinal opacities . of changes . left congestion airspace costophrenic scarring spine small process acute lateral to . the . exclude consistent appearance endplate                       2.841918\n",
       "disease identified size . of and evidence cardiopulmonary pulmonary                                                                                                                                2.486679\n",
       "                                                                                                                                                                                                     ...   \n",
       ". of mediastinal opacities . of changes . the left upper please exclude . the left upper consolidation . the . the . acute left congestion                                                         0.177620\n",
       ". of mediastinal opacities . of changes . left congestion airspace emphysema right acute left upper airspace . represent heart lower apical mild correlation lower cardiac                         0.177620\n",
       "disease identified size . left lung infiltrate size disease less left congestion airspace calcified . the left upper lung . of changes . the . the                                                 0.177620\n",
       ". the left upper lung otherwise . most apical line outside defined mild notification joint recommended . the . ct space elevated . the . the                                                       0.177620\n",
       ". acute view device left study heart spine mild be prior at . the perihilar study heart spine mild be interstitial acute lung . the .                                                              0.177620\n",
       "Name: count, Length: 299, dtype: float64"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test['prediction_gs'].value_counts()*100/test.shape[0] #greedy search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
