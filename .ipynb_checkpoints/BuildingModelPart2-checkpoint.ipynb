{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "475222bb-a670-4c46-b371-9e880f1936ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import tarfile\n",
    "import datetime\n",
    "import warnings\n",
    "import prettytable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import concat\n",
    "from tensorflow import repeat\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.utils import shuffle\n",
    "from prettytable import PrettyTable\n",
    "from skimage.transform import resize\n",
    "import nltk.translate.bleu_score as bleu\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.backend import expand_dims \n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import concatenate, Concatenate\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from tensorflow.keras.layers import Input, Softmax, RNN, Dense, Embedding, LSTM, Layer, Dropout, GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e21f5f4d-43e4-4a70-81f4-527fa94d4fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = DenseNet121(weights='CheXNet_weights.h5', classes = 14, input_shape=(256,256,3))\n",
    "model = Model(image_model.input, image_model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e95483c-b097-40af-9e97-c5f3487b78c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"CheXNet.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4de3ea0-587d-4d3d-b0a6-319fc402f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load('train.npy',allow_pickle=True)\n",
    "test = np.load('test.npy',allow_pickle=True)\n",
    "validation = np.load('validation.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b853fca4-cebf-4c40-8496-96fce4d1d46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"front X-Ray\", \"lateral X-Ray\", \"findings\", \"dec_ip\", \"dec_op\", \"image_features\"]\n",
    "\n",
    "train = pd.DataFrame(train, columns = columns)\n",
    "test = pd.DataFrame(test, columns = columns)\n",
    "validation = pd.DataFrame(validation, columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32ae66a3-c17f-447c-b23a-63edf08161b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data -      (3200, 6)\n",
      "test data -       (399, 6)\n",
      "validation data - (350, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data -     \",train.shape)\n",
    "print('test data -      ',test.shape)\n",
    "print('validation data -',validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8c747ff-864d-497c-9f7a-b377fe90566e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>front X-Ray</th>\n",
       "      <th>lateral X-Ray</th>\n",
       "      <th>findings</th>\n",
       "      <th>dec_ip</th>\n",
       "      <th>dec_op</th>\n",
       "      <th>image_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CXR677_IM-2249-2001.png</td>\n",
       "      <td>CXR677_IM-2249-1001.png</td>\n",
       "      <td>&lt;start&gt; picc line catheter tip in the right at...</td>\n",
       "      <td>&lt;start&gt; picc line catheter tip in the right at...</td>\n",
       "      <td>picc line catheter tip in the right atrium hea...</td>\n",
       "      <td>[[1.3660428521689028e-05, 0.001198855228722095...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CXR1096_IM-0066-2001.png</td>\n",
       "      <td>CXR1096_IM-0066-2001.png</td>\n",
       "      <td>&lt;start&gt; the heart size is within normal limits...</td>\n",
       "      <td>&lt;start&gt; the heart size is within normal limits...</td>\n",
       "      <td>the heart size is within normal limits no foca...</td>\n",
       "      <td>[[0.00015775891370140016, 0.001071828184649348...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CXR143_IM-0276-2001.png</td>\n",
       "      <td>CXR143_IM-0276-1001.png</td>\n",
       "      <td>&lt;start&gt; the lungs are clear no pleural effusio...</td>\n",
       "      <td>&lt;start&gt; the lungs are clear no pleural effusio...</td>\n",
       "      <td>the lungs are clear no pleural effusion is see...</td>\n",
       "      <td>[[0.00012958170555066317, 0.001051775994710624...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CXR3848_IM-1946-1001-0001.png</td>\n",
       "      <td>CXR3848_IM-1946-1001-0002.png</td>\n",
       "      <td>&lt;start&gt; the heart size is moderate to severely...</td>\n",
       "      <td>&lt;start&gt; the heart size is moderate to severely...</td>\n",
       "      <td>the heart size is moderate to severely enlarge...</td>\n",
       "      <td>[[0.0003334000357426703, 0.0013413153355941176...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CXR317_IM-1493-1001.png</td>\n",
       "      <td>CXR317_IM-1493-2001.png</td>\n",
       "      <td>&lt;start&gt; normal heart size normal mediastinal s...</td>\n",
       "      <td>&lt;start&gt; normal heart size normal mediastinal s...</td>\n",
       "      <td>normal heart size normal mediastinal silhouett...</td>\n",
       "      <td>[[0.00026678695576265454, 0.001648269011639058...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     front X-Ray                  lateral X-Ray  \\\n",
       "0        CXR677_IM-2249-2001.png        CXR677_IM-2249-1001.png   \n",
       "1       CXR1096_IM-0066-2001.png       CXR1096_IM-0066-2001.png   \n",
       "2        CXR143_IM-0276-2001.png        CXR143_IM-0276-1001.png   \n",
       "3  CXR3848_IM-1946-1001-0001.png  CXR3848_IM-1946-1001-0002.png   \n",
       "4        CXR317_IM-1493-1001.png        CXR317_IM-1493-2001.png   \n",
       "\n",
       "                                            findings  \\\n",
       "0  <start> picc line catheter tip in the right at...   \n",
       "1  <start> the heart size is within normal limits...   \n",
       "2  <start> the lungs are clear no pleural effusio...   \n",
       "3  <start> the heart size is moderate to severely...   \n",
       "4  <start> normal heart size normal mediastinal s...   \n",
       "\n",
       "                                              dec_ip  \\\n",
       "0  <start> picc line catheter tip in the right at...   \n",
       "1  <start> the heart size is within normal limits...   \n",
       "2  <start> the lungs are clear no pleural effusio...   \n",
       "3  <start> the heart size is moderate to severely...   \n",
       "4  <start> normal heart size normal mediastinal s...   \n",
       "\n",
       "                                              dec_op  \\\n",
       "0  picc line catheter tip in the right atrium hea...   \n",
       "1  the heart size is within normal limits no foca...   \n",
       "2  the lungs are clear no pleural effusion is see...   \n",
       "3  the heart size is moderate to severely enlarge...   \n",
       "4  normal heart size normal mediastinal silhouett...   \n",
       "\n",
       "                                      image_features  \n",
       "0  [[1.3660428521689028e-05, 0.001198855228722095...  \n",
       "1  [[0.00015775891370140016, 0.001071828184649348...  \n",
       "2  [[0.00012958170555066317, 0.001051775994710624...  \n",
       "3  [[0.0003334000357426703, 0.0013413153355941176...  \n",
       "4  [[0.00026678695576265454, 0.001648269011639058...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62ae86e7-c4e4-4291-860b-7460c2aeb07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size -  1465\n",
      "[[  5 364 310 188 148  22   1  26 279  15   3 143  91 169   8 709  52   7\n",
      "   16   4 114 242 102   2  12  13   4 103 117  11 192 161  22   1  30  79\n",
      "   33  13   3  45 710 247  11 192 161  22   1  26 218   8  79  16  35  26\n",
      "  170   9  14   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "token = Tokenizer( filters='!\"#$%&()*+,-/:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "token.fit_on_texts(train['findings'].astype(str))\n",
    "\n",
    "token.word_index['<pad>'] = 0\n",
    "token.index_word[0] = '<pad>'\n",
    "vocab_size = len(token.word_index) + 1\n",
    "print('Vocab size - ', vocab_size)\n",
    "\n",
    "#sequence in train and validation\n",
    "train_inp_dec = token.texts_to_sequences(train.dec_ip)\n",
    "train_op_dec = token.texts_to_sequences(train.dec_op)\n",
    "val_inp_dec = token.texts_to_sequences(validation.dec_ip)\n",
    "val_op_dec = token.texts_to_sequences(validation.dec_op)\n",
    "\n",
    "#padding in the train and validation \n",
    "max_len = 100\n",
    "decoder_input = pad_sequences(train_inp_dec, maxlen=max_len, padding='post')\n",
    "decoder_output =  pad_sequences(train_op_dec, maxlen=max_len, padding='post') \n",
    "Validation_decoder_input = pad_sequences(val_inp_dec, maxlen=max_len, padding='post') \n",
    "Validation_decoder_output = pad_sequences(val_op_dec, maxlen=max_len, padding='post')\n",
    "print(decoder_input[:1])\n",
    "\n",
    "word_idx = {}\n",
    "idx_word = {}\n",
    "for key, value in (token.word_index).items(): \n",
    "    word_idx[key] = value\n",
    "    idx_word[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d65b1bc5-1276-423b-84dd-330ff495befd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    '''\n",
    "    Encoder model -- That takes a input sequence and returns output sequence\n",
    "    '''\n",
    "    def __init__(self,lstm_units):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm_units = lstm_units\n",
    "        self.dense      = Dense(self.lstm_units, kernel_initializer=\"glorot_uniform\", name = 'encoder_dense_layer')\n",
    "        \n",
    "    def initialize_states(self,batch_size):\n",
    "      \n",
    "      self.batch_size  = batch_size\n",
    "      self.enc_h       = tf.zeros((self.batch_size, self.lstm_units))\n",
    "      \n",
    "      return self.enc_h\n",
    "    \n",
    "    def call(self,x):\n",
    "      \n",
    "      # x : image_data\n",
    "      encoder_output = self.dense(x)\n",
    "      \n",
    "      return encoder_output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69d1254f-008c-4f34-a99d-fe68b7a2c8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "  '''\n",
    "    Class that calculates score based on the scoring_function using Bahdanu attention mechanism.\n",
    "  '''\n",
    "  def __init__(self,attention_units):\n",
    "    super().__init__()\n",
    "\n",
    "    self.attention_units = attention_units  \n",
    "\n",
    "    self.w1_Dense    =  tf.keras.layers.Dense(self.attention_units, kernel_initializer=\"glorot_uniform\", name='Concat_w1_Dense')\n",
    "    self.w2_Dense    =  tf.keras.layers.Dense(self.attention_units, kernel_initializer=\"glorot_uniform\", name='Concat_w2_Dense')\n",
    "    self.Concat_Dense=  tf.keras.layers.Dense(1, kernel_initializer=\"glorot_uniform\", name = 'Concat_Dense_layer')\n",
    "  \n",
    "  def call(self,x):\n",
    "    \n",
    "    self.decoder_hidden_state, self.encoder_output = x\n",
    "    self.decoder_hidden_state = tf.expand_dims(self.decoder_hidden_state,axis = 1)\n",
    "    \n",
    "    score = self.Concat_Dense(tf.nn.tanh(self.w1_Dense(self.decoder_hidden_state) + self.w2_Dense(self.encoder_output)))\n",
    "    \n",
    "    att_weights    = tf.nn.softmax(score, axis=1)\n",
    "    context_vector = att_weights * self.encoder_output\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)   \n",
    "    \n",
    "    return context_vector,att_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59727e4a-bdd5-4524-aca6-61dca7a9ae30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStepDecoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, lstm_units, attention_units):\n",
    "      super().__init__()\n",
    "      \n",
    "      self.lstm_units     = lstm_units\n",
    "      self.vocab_size     = vocab_size\n",
    "      self.embedding_dim  = embedding_dim\n",
    "      self.attention_units= attention_units\n",
    "      \n",
    "      self.dense       = Dense(self.vocab_size, kernel_initializer=\"glorot_uniform\", name ='onestep_dense')\n",
    "      self.attention   = Attention( self.attention_units)\n",
    "      self.decoder_emb = Embedding(self.vocab_size, self.embedding_dim, trainable = True , name = 'Decoder_embedding')           \n",
    "      self.decoder_gru = GRU(self.lstm_units, return_state=True, return_sequences=True, name=\"Decoder_LSTM\") \n",
    "      \n",
    "      \n",
    "      self.dropout1 = Dropout(0.3,name = 'dropout1')\n",
    "      self.dropout2 = Dropout(0.3,name = 'dropout2')\n",
    "      self.dropout3 = Dropout(0.3,name = 'dropout3')\n",
    "      self.count = 0\n",
    "  \n",
    "\n",
    "  def call(self, x, training=None):\n",
    "    self.count += 1\n",
    "    \n",
    "    self.input_to_decoder, self.encoder_output, self.state_h = x\n",
    "    \n",
    "    embedded_output = self.decoder_emb(self.input_to_decoder)\n",
    "    \n",
    "    embedded_output = self.dropout1(embedded_output)\n",
    "      \n",
    "    y = [self.state_h, self.encoder_output]\n",
    "    context_vector, att_weights = self.attention(y)\n",
    "    \n",
    "    concated_decoder_input = tf.concat([tf.expand_dims(context_vector, 1), embedded_output], -1)\n",
    "    \n",
    "    concated_decoder_input = self.dropout2(concated_decoder_input)\n",
    "\n",
    "    output_gru, hidden_state = self.decoder_gru(concated_decoder_input, initial_state=self.state_h)\n",
    "    \n",
    "    output_gru = tf.reshape(output_gru, (-1, output_gru.shape[2]))\n",
    "    \n",
    "    output_gru = self.dropout3(output_gru)\n",
    "    \n",
    "    output = self.dense(output_gru)\n",
    "    \n",
    "    temp = [output, hidden_state, att_weights, context_vector]\n",
    "    \n",
    "    return [output, hidden_state, att_weights, context_vector]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea5b47db-2455-44c5-9bf5-7ae92f75a2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_units, attention_units):\n",
    "      super().__init__()\n",
    "\n",
    "      self.lstm_units     = lstm_units\n",
    "      self.vocab_size     = vocab_size\n",
    "      self.embedding_dim  = embedding_dim\n",
    "      self.attention_units= attention_units\n",
    "      self.onestepdecoder = OneStepDecoder(self.vocab_size, self.embedding_dim, self.lstm_units, self.attention_units)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x,training=None):\n",
    "        self.input_to_decoder, self.encoder_output, self.decoder_hidden_state = x\n",
    "        all_outputs = tf.TensorArray(tf.float32,size = self.input_to_decoder.shape[1], name = 'output_arrays' )\n",
    "        for timestep in tf.range(self.input_to_decoder.shape[1]):\n",
    "          y = [self.input_to_decoder[:,timestep:timestep+1],self.encoder_output, self.decoder_hidden_state]\n",
    "          output,hidden_state,att_weights,context_vector = self.onestepdecoder(y)\n",
    "          self.decoder_hidden_state = hidden_state\n",
    "          all_outputs = all_outputs.write(timestep,output)\n",
    "        all_outputs = tf.transpose(all_outputs.stack(),[1,0,2])\n",
    "        return all_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00411db3-943d-4ff2-a8eb-d41cdb287041",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_decoder(tf.keras.Model):\n",
    "  \"\"\"\n",
    "     # Intialize encoder states, Pass the encoder_sequence to the embedding layer\n",
    "     # Decoder initial states are encoder final states, Initialize it accordingly\n",
    "     # Pass the decoder sequence,encoder_output,decoder states to Decoder\n",
    "     # return the decoder output\n",
    "  \n",
    "  \"\"\" \n",
    "  def __init__(self, vocab_size, embedding_dim, lstm_units, attention_units, batch_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size     = vocab_size\n",
    "        self.batch_size     = batch_size\n",
    "        self.lstm_units     = lstm_units\n",
    "        self.embedding_dim  = embedding_dim\n",
    "        self.attention_units= attention_units\n",
    "        \n",
    "        self.encoder = Encoder(self.lstm_units)\n",
    "        self.decoder = Decoder(vocab_size, embedding_dim, lstm_units, attention_units)\n",
    "        self.dense   = Dense(self.vocab_size, kernel_initializer=\"glorot_uniform\", name = 'enc_dec_dense')\n",
    "\n",
    "\n",
    "  \n",
    "  def call(self,data):\n",
    "    \n",
    "    self.inputs, self.outputs = data[0], data[1]\n",
    "\n",
    "    self.encoder_hidden = self.encoder.initialize_states(self.batch_size)\n",
    "    self.encoder_output = self.encoder(self.inputs)\n",
    "    \n",
    "    x = [self.outputs,self.encoder_output,self.encoder_hidden]\n",
    "    output = self.decoder(x)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "131b74ec-5d53-4eb6-af2b-99bc507e779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40d1c157-85ff-463c-b1cf-0b2a2dd3bf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_units     = 256\n",
    "batch_size     = 50\n",
    "Buffer_size    = 500\n",
    "embedding_dim  = 300\n",
    "attention_units= 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eeb0b462-844f-483f-aa24-27f6a1ab609d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mannu\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:73: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "#This will clear all tensorflow session\n",
    "Attention_model = Encoder_decoder(vocab_size,embedding_dim,lstm_units,attention_units,batch_size)\n",
    "Attention_model.compile(optimizer=tf.keras.optimizers.Adam(100.0),loss=loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e11b0d50-e2d6-410b-b466-7e8745130f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2eb4296e-735c-49aa-9b45-d0eddc1ccc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',  patience = 10, baseline=None, verbose = 1, restore_best_weights=True)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, mode = 'min',verbose = 1, patience=5, min_lr=0.000001)\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b921d07b-757a-4aa9-8dfb-6603e7e6a1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping the Image tensors for training\n",
    "train_image_features = np.vstack(train.image_features).astype(float)\n",
    "validation_image_features = np.vstack(validation.image_features).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e35eb03f-039a-4faa-a889-d860732bf88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(((train_image_features, decoder_input), decoder_output))\n",
    "train_dataset = train_dataset.shuffle(Buffer_size).batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices(((validation_image_features,Validation_decoder_input),Validation_decoder_output))\n",
    "validation_dataset = validation_dataset.shuffle(Buffer_size).batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a61938f-cdf3-4343-a74e-3c2b359629b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 610ms/step - loss: 1.9485 - val_loss: 2.0538 - learning_rate: 10.0000\n",
      "Epoch 2/5\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 615ms/step - loss: 1.9263 - val_loss: 2.0499 - learning_rate: 10.0000\n",
      "Epoch 3/5\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 640ms/step - loss: 1.9238 - val_loss: 2.0499 - learning_rate: 10.0000\n",
      "Epoch 4/5\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 622ms/step - loss: 1.9522 - val_loss: 2.0499 - learning_rate: 10.0000\n",
      "Epoch 5/5\n",
      "\u001b[1m56/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 591ms/step - loss: 1.9344"
     ]
    }
   ],
   "source": [
    "Attention_model.fit(train_dataset, validation_data=validation_dataset, epochs=5, callbacks=[early_stop,reduce_lr,tensorboard], shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df3b1cf-904e-415d-82d9-23a540256f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_name):\n",
    "  \"\"\"Loads image in array format\"\"\"\n",
    "\n",
    "  image = Image.open(img_name)\n",
    "  X = np.asarray(image.convert(\"RGB\"))\n",
    "  X = np.asarray(X)\n",
    "  X = preprocess_input(X)\n",
    "  X = resize(X, (256,256,3))\n",
    "  X = np.expand_dims(X, axis=0)\n",
    "  X = np.asarray(X)\n",
    "    \n",
    "  return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe6a494-81d7-4e65-8a55-c4c6a9dc777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image1_paths,image2_paths):\n",
    "\n",
    "  '''\n",
    "    input -- dataframe(df)\n",
    "    output -- dataframe(df)\n",
    "    process - convert images into 256 X 256, then using CHeXNET model generate tensor(concate two image tensor)\n",
    "  \n",
    "  '''\n",
    "  path = 'images/'\n",
    "  image_features = []\n",
    "  for i in range(len(image1_paths)):\n",
    "\n",
    "    i1 = load_image(path+image1_paths)\n",
    "    i2 = load_image(path+image2_paths)\n",
    "    img1_features = model.predict(i1)\n",
    "    img2_features = model.predict(i2)\n",
    "    img1_features = np.vstack(img1_features).astype(float)\n",
    "    img2_features = np.vstack(img2_features).astype(float)\n",
    "    \n",
    "    tensor = np.concatenate((img1_features, img2_features), axis=1)\n",
    "\n",
    "  return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8bff64-077b-4e9b-889d-ce0cd82006a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refrence : https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/\n",
    "\n",
    "def evaluate(image1, image2):\n",
    "    '''\n",
    "    Input - two image and image path\n",
    "    output - return medical report of the images\n",
    "    This function takes images and using encoder decoder model\n",
    "    return medical report of the images\n",
    "    The function predicts the sentence using beam search\n",
    "\n",
    "    '''\n",
    "    img_tensor     = preprocess(image1, image2)\n",
    "    image_features = np.vstack(img_tensor).astype(float)\n",
    "\n",
    "    result = ''\n",
    "    initial_state = Attention_model.layers[0].initialize_states(1)\n",
    "    sequences     = [['<start>', initial_state, 0]]\n",
    "\n",
    "    encoder_output       = Attention_model.layers[0](image_features)\n",
    "    decoder_hidden_state = initial_state\n",
    "\n",
    "    max_len = 75\n",
    "    beam_width = 3\n",
    "    finished_seq = []\n",
    "\n",
    "    for i in range(max_len):#traverse through all lengths\n",
    "        new_seq = [] #stores the seq which does not have <end> in them\n",
    "        all_probable = [] #stores all the top k seq along with their scores\n",
    "        \n",
    "        for seq,state,score in sequences: #traverse for all top k sequences\n",
    "            \n",
    "            cur_vec = np.reshape(word_idx[seq.split(\" \")[-1]],(1,1)) \n",
    "            decoder_hidden_state = state\n",
    "            x = [cur_vec, encoder_output, decoder_hidden_state]\n",
    "            output,hidden_state,att_weights,context_vector = Attention_model.get_layer('decoder').onestepdecoder(x)\n",
    "            output = tf.nn.softmax(output)\n",
    "            top_words = np.argsort(output).flatten()[-beam_width:] #get the top k words\n",
    "            for index in top_words:\n",
    "                #here we will update score with log of probabilities and subtracting(log of prob will be in negative)\n",
    "                #here since its -(log), lower the score higher the prob\n",
    "                predicted = [seq + ' '+ idx_word[index], hidden_state, score-np.log(np.array(output).flatten()[index])] #updating the score and seq\n",
    "                all_probable.append(predicted)\n",
    "\n",
    "        sequences = sorted(all_probable, key = lambda l: l[2])[:beam_width] #getting the top 3 sentences with high prob ie low score\n",
    "        \n",
    "        count = 0 \n",
    "        for seq,state,score in sequences: #check for 'endseq' in each seq in the beam\n",
    "            if seq.split(\" \")[-1] == '<end>': #if last word of the seq is <end>\n",
    "                score = score/len(seq) #normalizing\n",
    "                finished_seq.append([seq,state,score])\n",
    "                count+=1\n",
    "            else:\n",
    "                new_seq.append([seq,state,score])\n",
    "        \n",
    "        sequences = new_seq\n",
    "        beam_width= beam_width - count #substracting the no. of finished sentences from beam length\n",
    "        \n",
    "        if not sequences: #if all the sequences reaches its end\n",
    "            break        \n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    if len(finished_seq) >0:\n",
    "          finished_seq = sorted(finished_seq, reverse=True, key = lambda l: l[2]) # Reverse Sorted the Predicted output by score measure\n",
    "          sequences = finished_seq[-1] #getting the last predicted output with least probablity score.\n",
    "          k=0\n",
    "          for i in finished_seq:\n",
    "            k+=1\n",
    "            print('Beam probablity-',i[2],'Candidate',k,':',i[0]) #printing top k predicted sentence by beam serch and their probablity score\n",
    "          return sequences[0]\n",
    "    else:\n",
    "          return new_seq[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2838737-eded-4d47-b608-8720d0e0b851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_img_cap(img_data):\n",
    "    \n",
    "    '''\n",
    "    input - imagedata point contain two x ray image and acutal medical report of the images\n",
    "    output - function return two images and its original and predical medical report\n",
    "    also return bleu score of the context\n",
    "    \n",
    "    '''\n",
    "    path = 'images/'\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize = (10,10), tight_layout=True)\n",
    "    count = 0\n",
    "    for img, subplot in zip(img_data[:2], axs.flatten()):\n",
    "        img_= mpimg.imread(path+img)\n",
    "        imgplot = axs[count].imshow(img_, cmap = 'bone')\n",
    "        count +=1\n",
    "    plt.show()\n",
    "    \n",
    "    print('Acutal Report :', img_data[2])\n",
    "    print('*'*200)\n",
    "    result = evaluate(img_data[0],img_data[1])\n",
    "    print('*'*200)\n",
    "    print(\"Best Predicted:\",result)\n",
    "    print('BLEU Score :-',sentence_bleu(img_data[2], result),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506c8824-fe0e-413a-b4e2-ebf0796b021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test.values[17:20]:\n",
    "    test_img_cap(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac83828d-17a5-40f6-a8a3-9e84dacac169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
